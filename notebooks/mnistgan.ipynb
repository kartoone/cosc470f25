{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d8568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Models\n",
    "# ---------------------------\n",
    "class Generator(nn.Module):\n",
    "    # Outputs 28x28 from latent z of size nz\n",
    "    def __init__(self, *, nz: int = 100, ngf: int = 64, nc: int = 1):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # z -> 512x7x7\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 7, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512x7x7 -> 256x14x14\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256x14x14 -> 128x28x28\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128x28x28 -> nc x 28x28\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 3, 1, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # Returns a single logit per image\n",
    "    def __init__(self, *, nc: int = 1, ndf: int = 64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 28x28 -> 14x14\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 14x14 -> 7x7\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 7x7 -> 1x1 (logit channel)\n",
    "            nn.Conv2d(ndf * 2, 1, 7, 1, 0, bias=False),\n",
    "            # No Sigmoid (we use BCEWithLogitsLoss)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)          # [B,1,1,1]\n",
    "        return out.view(-1, 1)      # [B,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e33243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Using nc=1, nz=100, ngf=64, ndf=64\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 1, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "Sanity checks passed.\n",
      "Starting Training Loop...\n",
      "[1/10][100/6000] Loss_D: 1.2234 (R 1.0409 F 0.1825) Loss_G: 1.9656\n"
     ]
    }
   ],
   "source": [
    "seed = 888\n",
    "nc=1\n",
    "nz=100\n",
    "ngf = 64\n",
    "batch_size = 10\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "data_root = \"/workspaces/cosc470/data\"\n",
    "\n",
    "set_seed(seed)\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Using nc={nc}, nz={nz}, ngf={ngf}, ndf={ndf}\")\n",
    "\n",
    "# Data\n",
    "tfm = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.5,), (0.5,)),  # scale to [-1, 1]\n",
    "])\n",
    "dataset = datasets.MNIST(root=data_root, train=True, download=True, transform=tfm)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "# Models\n",
    "netG = Generator(nz=nz, ngf=ngf, nc=nc).to(device)\n",
    "netD = Discriminator(nc=nc, ndf=ndf).to(device)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netG)\n",
    "print(netD)\n",
    "\n",
    "# Quick sanity on shapes\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4, nz, 1, 1, device=device)\n",
    "    fake = netG(z)\n",
    "    assert fake.shape == (4, nc, 28, 28), f\"G output shape wrong: {fake.shape}\"\n",
    "    x = torch.randn(4, nc, 28, 28, device=device)\n",
    "    y = netD(x)\n",
    "    assert y.shape == (4, 1), f\"D output shape wrong: {y.shape}\"\n",
    "    print(\"Sanity checks passed.\")\n",
    "\n",
    "# Loss & Optims\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Fixed noise for monitoring\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Output dir\n",
    "out_dir = Path(\"samples\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Labels (shape [B,1])\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (real, _) in enumerate(dataloader, start=1):\n",
    "        bsz = real.size(0)\n",
    "        real = real.to(device, non_blocking=True)\n",
    "\n",
    "        # -----------------\n",
    "        # Train Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # -----------------\n",
    "        netD.zero_grad(set_to_none=True)\n",
    "\n",
    "        labels_real = torch.full((bsz, 1), real_label, device=device, dtype=torch.float32)\n",
    "        out_real = netD(real)                  # [B,1]\n",
    "        lossD_real = criterion(out_real, labels_real)\n",
    "        lossD_real.backward()\n",
    "\n",
    "        noise = torch.randn(bsz, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)                     # [B,1,28,28]\n",
    "        labels_fake = torch.full((bsz, 1), fake_label, device=device, dtype=torch.float32)\n",
    "        out_fake = netD(fake.detach())         # [B,1]\n",
    "        lossD_fake = criterion(out_fake, labels_fake)\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        optimizerD.step()\n",
    "        lossD = lossD_real + lossD_fake\n",
    "\n",
    "        # -----------------\n",
    "        # Train Generator: maximize log(D(G(z)))  (i.e., fool D to think fakes are real)\n",
    "        # -----------------\n",
    "        netG.zero_grad(set_to_none=True)\n",
    "        labels_gen = torch.full((bsz, 1), real_label, device=device, dtype=torch.float32)\n",
    "        out_fake_for_G = netD(fake)            # recompute; do not detach\n",
    "        lossG = criterion(out_fake_for_G, labels_gen)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{epoch}/{epochs}][{i}/{len(dataloader)}] \"\n",
    "                    f\"Loss_D: {lossD.item():.4f} (R {lossD_real.item():.4f} F {lossD_fake.item():.4f}) \"\n",
    "                    f\"Loss_G: {lossG.item():.4f}\")\n",
    "\n",
    "    # Save a grid each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_fixed = netG(fixed_noise).cpu()\n",
    "    grid = vutils.make_grid(fake_fixed, padding=2, normalize=True, nrow=8)\n",
    "    vutils.save_image(grid, out_dir / f\"epoch_{epoch:03d}.png\")\n",
    "\n",
    "# Final sample\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, nz, 1, 1, device=device)\n",
    "    final_fake = netG(z).cpu()\n",
    "    vutils.save_image(vutils.make_grid(final_fake, padding=2, normalize=True, nrow=8),\n",
    "                    out_dir / \"final.png\")\n",
    "print(f\"Done. Samples saved to: {out_dir.resolve()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed32bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
